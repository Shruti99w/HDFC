{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f4bcca-6f29-488a-95b9-6be486de65a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer duplicate columns: []\nLoan duplicate columns: []\nDisbursement duplicate columns: []\nFinal record counts\nCustomers: 30\nLoans: 41\nDisbursements: 29\nBranches: 8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ONE-CELL DATA QUALITY CORRECTION PIPELINE (SILVER-READY)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD RAW DATA\n",
    "# ------------------------------------------------------------\n",
    "customers_raw= spark.read.option(\"header\",True).csv(\"/Volumes/hdfc_data_mentor/etl/shrutivolume/customer.csv\")\n",
    "branches_raw= spark.read.option(\"header\",True).csv(\"/Volumes/hdfc_data_mentor/etl/shrutivolume/branches.csv\")\n",
    "loans_raw=spark.read.json(\"/Volumes/hdfc_data_mentor/etl/shrutivolume/loans.json\")\n",
    "disb_raw=spark.read.parquet(\"/Volumes/hdfc_data_mentor/etl/shrutivolume/disbursements.parquet\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. NULL HANDLING (MANDATORY FIELDS)\n",
    "# ------------------------------------------------------------\n",
    "customers_valid = customers_raw.filter(\"customer_id IS NOT NULL\")\n",
    "loans_valid = loans_raw.filter(\"loan_id IS NOT NULL AND customer_id IS NOT NULL\")\n",
    "disb_valid = disb_raw.filter(\"loan_id IS NOT NULL\")\n",
    "branches_valid = branches_raw.filter(\"branch_id IS NOT NULL\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. DATA TYPE & ENCODING FIXES\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Fix comma-separated numeric values\n",
    "loans_valid = loans_valid.withColumn(\n",
    "    \"loan_amount\",\n",
    "    F.regexp_replace(\"loan_amount\", \",\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. TIMESTAMP STANDARDIZATION (FINAL SAFE FIX)\n",
    "# ------------------------------------------------------------\n",
    "# Handles:\n",
    "# yyyy-MM-dd HH:mm:ss\n",
    "# dd-MM-yyyy HH:mm\n",
    "\n",
    "loans_valid = loans_valid.withColumn(\n",
    "    \"update_ts\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_timestamp(update_ts, 'yyyy-MM-dd HH:mm:ss')\"),\n",
    "        F.expr(\"try_to_timestamp(update_ts, 'dd-MM-yyyy HH:mm')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "customers_valid = customers_valid.withColumn(\n",
    "    \"update_ts\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_timestamp(update_ts, 'yyyy-MM-dd HH:mm:ss')\"),\n",
    "        F.expr(\"try_to_timestamp(update_ts, 'dd-MM-yyyy HH:mm')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "disb_valid = disb_valid.withColumn(\n",
    "    \"update_ts\",\n",
    "    F.coalesce(\n",
    "        F.expr(\"try_to_timestamp(update_ts, 'yyyy-MM-dd HH:mm:ss')\"),\n",
    "        F.expr(\"try_to_timestamp(update_ts, 'dd-MM-yyyy HH:mm')\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. DEDUPLICATION (DETERMINISTIC)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Customers: latest record per customer\n",
    "w_cust = Window.partitionBy(\"customer_id\").orderBy(F.col(\"update_ts\").desc_nulls_last())\n",
    "customers_clean = (\n",
    "    customers_valid\n",
    "    .withColumn(\"rn\", F.row_number().over(w_cust))\n",
    "    .filter(\"rn = 1\")\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# Loans: latest record per loan\n",
    "w_loan = Window.partitionBy(\"loan_id\").orderBy(F.col(\"update_ts\").desc_nulls_last())\n",
    "loans_clean = (\n",
    "    loans_valid\n",
    "    .withColumn(\"rn\", F.row_number().over(w_loan))\n",
    "    .filter(\"rn = 1\")\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# Disbursements: latest per loan\n",
    "w_disb = Window.partitionBy(\"loan_id\").orderBy(F.col(\"update_ts\").desc_nulls_last())\n",
    "disb_clean = (\n",
    "    disb_valid\n",
    "    .withColumn(\"rn\", F.row_number().over(w_disb))\n",
    "    .filter(\"rn = 1\")\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. CATEGORICAL STANDARDIZATION\n",
    "# ------------------------------------------------------------\n",
    "loans_clean = loans_clean.withColumn(\n",
    "    \"status\",\n",
    "    F.upper(F.trim(\"status\"))\n",
    ")\n",
    "\n",
    "valid_status = [\"APPROVED\", \"PENDING\", \"DISBURSED\", \"CLOSED\"]\n",
    "loans_clean = loans_clean.filter(F.col(\"status\").isin(valid_status))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. REFERENTIAL INTEGRITY\n",
    "# ------------------------------------------------------------\n",
    "loans_clean = loans_clean.join(\n",
    "    branches_valid.select(\"branch_id\"),\n",
    "    \"branch_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. ADD STANDARDIZED METADATA (OPTIONAL)\n",
    "# ------------------------------------------------------------\n",
    "customers_clean = customers_clean.withColumn(\"record_source\", F.lit(\"CUSTOMER_SYS\"))\n",
    "loans_clean = loans_clean.withColumn(\"record_source\", F.lit(\"LOAN_SYS\"))\n",
    "disb_clean = disb_clean.withColumn(\"record_source\", F.lit(\"DISBURSEMENT_SYS\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9. FINAL VALIDATION CHECKS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def find_duplicate_columns(df):\n",
    "    return [c for c, n in Counter(df.columns).items() if n > 1]\n",
    "\n",
    "print(\"Customer duplicate columns:\", find_duplicate_columns(customers_clean))\n",
    "print(\"Loan duplicate columns:\", find_duplicate_columns(loans_clean))\n",
    "print(\"Disbursement duplicate columns:\", find_duplicate_columns(disb_clean))\n",
    "\n",
    "print(\"Final record counts\")\n",
    "print(\"Customers:\", customers_clean.count())\n",
    "print(\"Loans:\", loans_clean.count())\n",
    "print(\"Disbursements:\", disb_clean.count())\n",
    "print(\"Branches:\", branches_valid.count())\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT DATAFRAMES (SILVER-READY)\n",
    "# customers_clean\n",
    "# loans_clean\n",
    "# disb_clean\n",
    "# branches_valid\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c48237-c634-414a-b3d2-dc6d71dc0a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert customers_clean.filter(\"customer_id IS NULL\").count() == 0, \"NULL customer_id in Silver\"\n",
    "assert loans_clean.filter(\"loan_id IS NULL OR customer_id IS NULL\").count() == 0, \"NULL keys in Silver loans\"\n",
    "assert disb_clean.filter(\"loan_id IS NULL\").count() == 0, \"NULL loan_id in Silver disbursements\"\n",
    "assert branches_valid.filter(\"branch_id IS NULL\").count() == 0, \"NULL branch_id in Silver branches\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8341d1c1-f8b3-4079-8849-4b15366fa18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert customers_clean.groupBy(\"customer_id\").count().filter(\"count > 1\").count() == 0, \"Duplicate customers in Silver\"\n",
    "assert loans_clean.groupBy(\"loan_id\").count().filter(\"count > 1\").count() == 0, \"Duplicate loans in Silver\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589f6ea7-13b1-4369-939d-4dc8fa4dc8cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_branch_refs = loans_clean.join(\n",
    "    branches_valid.select(\"branch_id\"),\n",
    "    \"branch_id\",\n",
    "    \"left_anti\"\n",
    ").count()\n",
    "\n",
    "assert invalid_branch_refs == 0, \"Broken branch reference in Silver loans\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e090d1-02b7-4e9a-ac82-f01a906037ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert loans_clean.filter(\"loan_amount < 0\").count() == 0, \"Negative loan amount found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4f0bb8-6d01-4b20-9e7b-24bf66449c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_clean.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"silver_customers\")\n",
    "\n",
    "loans_clean.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"silver_loans\")\n",
    "\n",
    "disb_clean.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"silver_disbursements\")\n",
    "\n",
    "branches_valid.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"silver_branches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ead7e1f-f04c-4f19-a1ec-1fec0a376a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"silver_customers\").count()\n",
    "spark.table(\"silver_loans\").count()\n",
    "spark.table(\"silver_disbursements\").count()\n",
    "spark.table(\"silver_branches\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b83036a-108d-4c94-939f-785046927597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_fact = spark.table(\"silver_loans\").alias(\"l\") \\\n",
    "    .join(spark.table(\"silver_customers\").alias(\"c\"), \"customer_id\", \"left\") \\\n",
    "    .join(spark.table(\"silver_disbursements\").alias(\"d\"), \"loan_id\", \"left\") \\\n",
    "    .join(spark.table(\"silver_branches\").alias(\"b\"), \"branch_id\", \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2973a3-8d8e-4140-93f1-d36b672139a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8338144220922009>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m silver_fact\u001B[38;5;241m.\u001B[39mwrite \\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msilver_customer_loan_fact\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:737\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n",
       "\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 737\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    738\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    739\u001B[0m )\n",
       "\u001B[1;32m    740\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [COLUMN_ALREADY_EXISTS] The column `city` already exists. Choose another name or rename the existing column. SQLSTATE: 42711\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.columnAlreadyExistsError(QueryCompilationErrors.scala:3851)\n",
       "\tat org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:179)\n",
       "\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:472)\n",
       "\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:279)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:277)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4092)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3450)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[COLUMN_ALREADY_EXISTS] The column `city` already exists. Choose another name or rename the existing column. SQLSTATE: 42711\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.columnAlreadyExistsError(QueryCompilationErrors.scala:3851)\n\tat org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:179)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:472)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:279)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:277)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4092)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3450)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "metadata": {
        "errorSummary": "[COLUMN_ALREADY_EXISTS] The column `city` already exists. Choose another name or rename the existing column. SQLSTATE: 42711"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "COLUMN_ALREADY_EXISTS",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "42711",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.columnAlreadyExistsError(QueryCompilationErrors.scala:3851)\n\tat org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:179)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:472)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:279)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:277)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4092)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3450)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-8338144220922009>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m silver_fact\u001B[38;5;241m.\u001B[39mwrite \\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msilver_customer_loan_fact\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:737\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 737\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    738\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    739\u001B[0m )\n\u001B[1;32m    740\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [COLUMN_ALREADY_EXISTS] The column `city` already exists. Choose another name or rename the existing column. SQLSTATE: 42711\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.columnAlreadyExistsError(QueryCompilationErrors.scala:3851)\n\tat org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:179)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:472)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:279)\n\tat org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:277)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:730)\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:854)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:702)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveAsTable(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4092)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3450)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "silver_fact.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"silver_customer_loan_fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ef4104-35eb-4862-b210-2b82963d5238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "l = spark.table(\"silver_loans\").alias(\"l\")\n",
    "c = spark.table(\"silver_customers\").alias(\"c\")\n",
    "d = spark.table(\"silver_disbursements\").alias(\"d\")\n",
    "b = spark.table(\"silver_branches\").alias(\"b\")\n",
    "\n",
    "silver_fact = (\n",
    "    l.join(c, \"customer_id\", \"left\")\n",
    "     .join(d, \"loan_id\", \"left\")\n",
    "     .join(b, \"branch_id\", \"left\")\n",
    "     .select(\n",
    "        # ---- Keys ----\n",
    "        l.loan_id,\n",
    "        l.customer_id,\n",
    "        l.branch_id,\n",
    "\n",
    "        # ---- Loan attributes ----\n",
    "        l.loan_amount,\n",
    "        l.interest_rate,\n",
    "        l.loan_type,\n",
    "        l.tenure_months,\n",
    "        l.status,\n",
    "        l.origination_date.alias(\"loan_origination_date\"),\n",
    "        l.update_ts.alias(\"loan_update_ts\"),\n",
    "\n",
    "        # ---- Customer attributes ----\n",
    "        c.full_name,\n",
    "        c.dob,\n",
    "        c.pan,\n",
    "        c.mobile,\n",
    "        c.email,\n",
    "        c.city.alias(\"customer_city\"),\n",
    "        c.annual_income,\n",
    "        c.update_ts.alias(\"customer_update_ts\"),\n",
    "\n",
    "        # ---- Disbursement attributes ----\n",
    "        d.disbursement_id,\n",
    "        d.disbursement_date,\n",
    "        d.disbursement_amount,\n",
    "        d.payment_mode,\n",
    "        d.update_ts.alias(\"disbursement_update_ts\"),\n",
    "\n",
    "        # ---- Branch attributes ----\n",
    "        b.branch_name,\n",
    "        b.city.alias(\"branch_city\"),\n",
    "        b.region,\n",
    "\n",
    "        # ---- Metadata ----\n",
    "        F.current_timestamp().alias(\"silver_ingest_ts\")\n",
    "     )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28657fe-ac1f-4f2e-a218-68a4692fc95f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_fact.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"silver_customer_loan_fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee7c000-0093-401b-a90e-1da9a059ebc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " #Gold needs to support:\n",
    " 1. customer 360 analytics\n",
    " 2.branch level performance\n",
    " 3.loan portfolio monitoring\n",
    " 4.disbursement performance\n",
    " \n",
    " #Load Silver Fact (Single Source of Truth)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver_fact = spark.table(\"silver_customer_loan_fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a4b5dc-e0d6-467b-9cf1-12a3c8aedc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_base = spark.table(\"silver_customer_loan_fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e03dda-74db-49c0-98d7-be82179ca84c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>loan_id</th><th>customer_id</th><th>branch_id</th><th>loan_amount</th><th>interest_rate</th><th>loan_type</th><th>tenure_months</th><th>status</th><th>loan_origination_date</th><th>loan_update_ts</th><th>full_name</th><th>dob</th><th>pan</th><th>mobile</th><th>email</th><th>customer_city</th><th>annual_income</th><th>customer_update_ts</th><th>disbursement_id</th><th>disbursement_date</th><th>disbursement_amount</th><th>payment_mode</th><th>disbursement_update_ts</th><th>branch_name</th><th>branch_city</th><th>region</th><th>silver_ingest_ts</th></tr></thead><tbody><tr><td>L50001</td><td>C1000</td><td>BR105</td><td>2057234.0</td><td>10.5</td><td>Personal</td><td>12</td><td>APPROVED</td><td>2025-11-26</td><td>2025-12-01T00:00:00.000Z</td><td>null</td><td>05-04-2007</td><td>JLBIB2876M</td><td>null</td><td>c1000@example.com</td><td>Chennai</td><td>443377</td><td>2025-11-15T00:00:00.000Z</td><td>D84436419</td><td>2025-12-01</td><td>2057234.0</td><td>RTGS</td><td>2025-12-04T00:00:00.000Z</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50002</td><td>C1001</td><td>BR106</td><td>1960486.0</td><td>8.5</td><td>Auto</td><td>48</td><td>PENDING</td><td>2025-11-23</td><td>2025-11-23T00:00:00.000Z</td><td>Kabir Singh</td><td>23-08-1979</td><td>NAWIQ1462F</td><td>9.18106E+11</td><td>c1001@example.com</td><td>Kolkata</td><td>2445150</td><td>2025-11-24T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Gachibowli</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50003</td><td>C1002</td><td>BR106</td><td>727030.0</td><td>9.2</td><td>Auto</td><td>48</td><td>PENDING</td><td>2025-11-18</td><td>2025-11-19T00:00:00.000Z</td><td>Meera Nair</td><td>17-04-2000</td><td>GBGRS7166F</td><td>9.16512E+11</td><td>c1002@example.com</td><td>Mumbai</td><td>1375791</td><td>2025-12-11T00:00:00.000Z</td><td>D37499621</td><td>2025-12-01</td><td>654327.0</td><td>IMPS</td><td>2025-11-20T00:00:00.000Z</td><td>Gachibowli</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50004</td><td>C1003</td><td>BR103</td><td>2421753.0</td><td>8.5</td><td>Home</td><td>60</td><td>PENDING</td><td>2025-11-21</td><td>2025-11-26T00:00:00.000Z</td><td>Vihaan Khan</td><td>02-09-2020</td><td>WTMHM5219V</td><td>9.19126E+11</td><td>c1003@example.com</td><td>Mumbai</td><td>1698696</td><td>2025-11-19T00:00:00.000Z</td><td>D64403047</td><td>2025-11-30</td><td>2421753.0</td><td>IMPS</td><td>2025-11-28T00:00:00.000Z</td><td>Indiranagar</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50005</td><td>C1004</td><td>BR104</td><td>1914865.0</td><td>10.5</td><td>Auto</td><td>12</td><td>PENDING</td><td>2025-11-13</td><td>2025-11-22T00:00:00.000Z</td><td>Ishaan Singh</td><td>14-03-1997</td><td>HFXEI9849G</td><td>9.16909E+11</td><td>c1004@example.com</td><td>Chennai</td><td>2247752</td><td>2025-11-23T00:00:00.000Z</td><td>D96808887</td><td>2025-11-13</td><td>1914865.0</td><td>NEFT</td><td>2025-11-23T00:00:00.000Z</td><td>Koramangala</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50006</td><td>C1005</td><td>BR105</td><td>1507901.0</td><td>11.0</td><td>Personal</td><td>24</td><td>PENDING</td><td>2025-11-17</td><td>2025-11-21T00:00:00.000Z</td><td>Aarav Singh</td><td>15-10-2015</td><td>EBTUK4244M</td><td>9.18527E+11</td><td>c1005@example.com</td><td>Hyderabad</td><td>1826176</td><td>2025-11-14T00:00:00.000Z</td><td>D29721737</td><td>2025-11-18</td><td>1357110.9</td><td>RTGS</td><td>2025-11-23T00:00:00.000Z</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50007</td><td>C1006</td><td>BR101</td><td>1435270.0</td><td>12.5</td><td>Home</td><td>36</td><td>APPROVED</td><td>2025-11-18</td><td>2025-11-22T00:00:00.000Z</td><td>Ananya Gupta</td><td>02-11-2022</td><td>OSSRD4960D</td><td>9.17783E+11</td><td>c1006@example.com</td><td>Kolkata</td><td>2259522</td><td>2025-11-12T00:00:00.000Z</td><td>D10346384</td><td>2025-12-01</td><td>1435270.0</td><td>RTGS</td><td>2025-11-26T00:00:00.000Z</td><td>Bandra</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50008</td><td>C1006</td><td>BR100</td><td>947531.0</td><td>10.5</td><td>Personal</td><td>36</td><td>CLOSED</td><td>2025-11-29</td><td>2025-12-07T00:00:00.000Z</td><td>Ananya Gupta</td><td>02-11-2022</td><td>OSSRD4960D</td><td>9.17783E+11</td><td>c1006@example.com</td><td>Kolkata</td><td>2259522</td><td>2025-11-12T00:00:00.000Z</td><td>D47948592</td><td>2025-12-12</td><td>947531.0</td><td>IMPS</td><td>2025-12-08T00:00:00.000Z</td><td>Andheri</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50009</td><td>C1007</td><td>BR102</td><td>260513.0</td><td>8.5</td><td>Auto</td><td>60</td><td>APPROVED</td><td>2025-11-10</td><td>2025-11-15T00:00:00.000Z</td><td>Riya Singh</td><td>07-12-1992</td><td>UKVEH0439T</td><td>9.17867E+11</td><td>null</td><td>Hyderabad</td><td>1479781</td><td>2025-11-25T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Powai</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50010</td><td>C1008</td><td>BR105</td><td>1751301.0</td><td>11.0</td><td>Education</td><td>48</td><td>PENDING</td><td>2025-11-11</td><td>2025-11-12T00:00:00.000Z</td><td>Ananya Das</td><td>29-08-2008</td><td>MMBHD7525N</td><td>9.1969E+11</td><td>c1008@example.com</td><td>Pune</td><td>1925714</td><td>2025-11-16T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50011</td><td>C1009</td><td>BR104</td><td>2046383.0</td><td>12.5</td><td>Auto</td><td>48</td><td>PENDING</td><td>2025-11-09</td><td>2025-11-12T00:00:00.000Z</td><td>Ananya Das</td><td>17-08-2019</td><td>JBNFR1197O</td><td>9.17482E+11</td><td>c1009@example.com</td><td>Chennai</td><td>2040279</td><td>2025-11-17T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Koramangala</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50012</td><td>C1010</td><td>BR103</td><td>2764833.0</td><td>9.2</td><td>Education</td><td>12</td><td>APPROVED</td><td>2025-11-22</td><td>2025-11-23T00:00:00.000Z</td><td>Saanvi Gupta</td><td>27-05-1981</td><td>PQCQL0958W</td><td>9.18746E+11</td><td>c1010@example.com</td><td>Kolkata</td><td>1320336</td><td>2025-11-12T00:00:00.000Z</td><td>D02837874</td><td>2025-12-04</td><td>2764833.0</td><td>RTGS</td><td>2025-11-24T00:00:00.000Z</td><td>Indiranagar</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50013</td><td>C1011</td><td>BR107</td><td>1238582.0</td><td>10.5</td><td>Home</td><td>60</td><td>APPROVED</td><td>2025-11-20</td><td>2025-11-29T00:00:00.000Z</td><td>Aarav Iyer</td><td>04-05-2021</td><td>TJXGC5407J</td><td>9.17749E+11</td><td>c1011@example.com</td><td>Mumbai</td><td>1666969</td><td>2025-12-07T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>SaltLake</td><td>Kolkata</td><td>East</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50014</td><td>C1011</td><td>BR103</td><td>767667.0</td><td>12.5</td><td>Home</td><td>48</td><td>PENDING</td><td>2025-11-12</td><td>2025-11-17T00:00:00.000Z</td><td>Aarav Iyer</td><td>04-05-2021</td><td>TJXGC5407J</td><td>9.17749E+11</td><td>c1011@example.com</td><td>Mumbai</td><td>1666969</td><td>2025-12-07T00:00:00.000Z</td><td>D59846102</td><td>2025-11-19</td><td>767667.0</td><td>IMPS</td><td>2025-11-20T00:00:00.000Z</td><td>Indiranagar</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50015</td><td>C1012</td><td>BR105</td><td>2975871.0</td><td>12.5</td><td>Personal</td><td>36</td><td>DISBURSED</td><td>2025-11-09</td><td>2025-11-09T00:00:00.000Z</td><td>Aditya Gupta</td><td>24-05-1990</td><td>QGGRD5607D</td><td>9.17741E+11</td><td>c1012@example.com</td><td>Bengaluru</td><td>2060719</td><td>2025-11-23T00:00:00.000Z</td><td>D53273166</td><td>2025-11-18</td><td>2975871.0</td><td>RTGS</td><td>2025-11-09T00:00:00.000Z</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50016</td><td>C1012</td><td>BR105</td><td>1418271.0</td><td>8.5</td><td>Education</td><td>60</td><td>CLOSED</td><td>2025-11-11</td><td>2025-11-11T00:00:00.000Z</td><td>Aditya Gupta</td><td>24-05-1990</td><td>QGGRD5607D</td><td>9.17741E+11</td><td>c1012@example.com</td><td>Bengaluru</td><td>2060719</td><td>2025-11-23T00:00:00.000Z</td><td>D43719185</td><td>2025-11-13</td><td>1418271.0</td><td>RTGS</td><td>2025-11-11T00:00:00.000Z</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50017</td><td>C1013</td><td>BR105</td><td>2694218.0</td><td>8.5</td><td>Home</td><td>36</td><td>DISBURSED</td><td>2025-11-30</td><td>2025-12-04T00:00:00.000Z</td><td>Sara Patel</td><td>22-10-1986</td><td>RKUNC0665I</td><td>9.1873E+11</td><td>c1013@mail.com</td><td>Hyderabad</td><td>2123657</td><td>2025-12-07T00:00:00.000Z</td><td>D65074561</td><td>2025-12-06</td><td>2694218.0</td><td>IMPS</td><td>2025-12-05T00:00:00.000Z</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50018</td><td>C1013</td><td>BR103</td><td>261919.0</td><td>11.0</td><td>Education</td><td>36</td><td>CLOSED</td><td>2025-11-12</td><td>2025-11-15T00:00:00.000Z</td><td>Sara Patel</td><td>22-10-1986</td><td>RKUNC0665I</td><td>9.1873E+11</td><td>c1013@mail.com</td><td>Hyderabad</td><td>2123657</td><td>2025-12-07T00:00:00.000Z</td><td>D23083211</td><td>2025-11-17</td><td>261919.0</td><td>IMPS</td><td>2025-11-18T00:00:00.000Z</td><td>Indiranagar</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50019</td><td>C1014</td><td>BR104</td><td>2262528.0</td><td>11.0</td><td>Auto</td><td>12</td><td>PENDING</td><td>2025-11-04</td><td>2025-11-13T00:00:00.000Z</td><td>Meera Reddy</td><td>13-05-1975</td><td>VQQNJ1765G</td><td>9.19187E+11</td><td>c1014@example.com</td><td>Mumbai</td><td>2020579</td><td>2025-11-18T00:00:00.000Z</td><td>D89490099</td><td>2025-11-08</td><td>2262528.0</td><td>RTGS</td><td>2025-11-14T00:00:00.000Z</td><td>Koramangala</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50020</td><td>C1014</td><td>BR102</td><td>874717.0</td><td>8.5</td><td>Personal</td><td>60</td><td>CLOSED</td><td>2025-12-04</td><td>2025-12-12T00:00:00.000Z</td><td>Meera Reddy</td><td>13-05-1975</td><td>VQQNJ1765G</td><td>9.19187E+11</td><td>c1014@example.com</td><td>Mumbai</td><td>2020579</td><td>2025-11-18T00:00:00.000Z</td><td>D57524236</td><td>2025-12-08</td><td>874717.0</td><td>IMPS</td><td>2025-12-15T00:00:00.000Z</td><td>Powai</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50021</td><td>C1015</td><td>BR100</td><td>701951.0</td><td>11.0</td><td>Home</td><td>60</td><td>CLOSED</td><td>2025-11-27</td><td>2025-11-27T00:00:00.000Z</td><td>Arjun Khan</td><td>28-08-1988</td><td>NLFGZ8057J</td><td>9.18223E+11</td><td>c1015@example.com</td><td>Hyderabad</td><td>1282560</td><td>2025-11-25T00:00:00.000Z</td><td>D15832328</td><td>2025-12-10</td><td>701951.0</td><td>RTGS</td><td>2025-11-30T00:00:00.000Z</td><td>Andheri</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50022</td><td>C1016</td><td>BR104</td><td>1068906.0</td><td>9.2</td><td>Personal</td><td>60</td><td>APPROVED</td><td>2025-11-10</td><td>2025-11-14T00:00:00.000Z</td><td>Ishaan Sharma</td><td>05-02-1987</td><td>COBXT5630Q</td><td>9.16458E+11</td><td>c1016@example.com</td><td>Chennai</td><td>1722464</td><td>2025-11-04T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Koramangala</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50023</td><td>C1017</td><td>BR105</td><td>602913.0</td><td>8.5</td><td>Personal</td><td>24</td><td>APPROVED</td><td>2025-11-16</td><td>2025-11-18T00:00:00.000Z</td><td>Saanvi Khan</td><td>09-11-2002</td><td>SLJKD7615W</td><td>9.16317E+11</td><td>c1017@example.com</td><td>Mumbai</td><td>1971247</td><td>2025-11-27T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50024</td><td>C1017</td><td>BR107</td><td>250291.0</td><td>11.0</td><td>Personal</td><td>24</td><td>PENDING</td><td>2025-11-05</td><td>2025-11-10T00:00:00.000Z</td><td>Saanvi Khan</td><td>09-11-2002</td><td>SLJKD7615W</td><td>9.16317E+11</td><td>c1017@example.com</td><td>Mumbai</td><td>1971247</td><td>2025-11-27T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>SaltLake</td><td>Kolkata</td><td>East</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50025</td><td>C1018</td><td>BR105</td><td>2345326.0</td><td>10.5</td><td>Auto</td><td>60</td><td>CLOSED</td><td>2025-11-25</td><td>2025-12-03T00:00:00.000Z</td><td>Meera Das</td><td>19-06-1979</td><td>SOSQF9918D</td><td>9.18237E+11</td><td>c1018@example.com</td><td>Chennai</td><td>1606386</td><td>2025-11-25T00:00:00.000Z</td><td>D19658626</td><td>2025-11-29</td><td>2345326.0</td><td>NEFT</td><td>2025-12-05T00:00:00.000Z</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50026</td><td>C1018</td><td>BR102</td><td>2203139.0</td><td>12.5</td><td>Personal</td><td>12</td><td>DISBURSED</td><td>2025-11-01</td><td>2025-11-05T00:00:00.000Z</td><td>Meera Das</td><td>19-06-1979</td><td>SOSQF9918D</td><td>9.18237E+11</td><td>c1018@example.com</td><td>Chennai</td><td>1606386</td><td>2025-11-25T00:00:00.000Z</td><td>D60486181</td><td>2025-11-08</td><td>2203139.0</td><td>RTGS</td><td>2025-11-05T00:00:00.000Z</td><td>Powai</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50027</td><td>C1019</td><td>BR104</td><td>2797175.0</td><td>12.5</td><td>Auto</td><td>24</td><td>PENDING</td><td>2025-11-20</td><td>2025-11-21T00:00:00.000Z</td><td>Aditya Patel</td><td>09-07-1996</td><td>WTHUB5577E</td><td>9.19931E+11</td><td>c1019@example.com</td><td>Chennai</td><td>732815</td><td>2025-11-06T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Koramangala</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50028</td><td>C1020</td><td>BR102</td><td>2056019.0</td><td>10.5</td><td>Education</td><td>24</td><td>APPROVED</td><td>2025-11-18</td><td>2025-11-27T00:00:00.000Z</td><td>Kabir Nair</td><td>16-09-1976</td><td>LEQPU9531T</td><td>9.16936E+11</td><td>null</td><td>Kolkata</td><td>2175259</td><td>2025-12-12T00:00:00.000Z</td><td>D63914701</td><td>2025-11-22</td><td>2056019.0</td><td>IMPS</td><td>2025-11-29T00:00:00.000Z</td><td>Powai</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50029</td><td>C1021</td><td>BR103</td><td>269606.0</td><td>8.5</td><td>Auto</td><td>48</td><td>CLOSED</td><td>2025-11-18</td><td>2025-11-26T00:00:00.000Z</td><td>Arjun Das</td><td>20-06-1989</td><td>SSSRC1170L</td><td>9.17737E+11</td><td>c1021@example.com</td><td>Hyderabad</td><td>1739312</td><td>2025-12-02T00:00:00.000Z</td><td>D15592542</td><td>2025-11-30</td><td>242645.4</td><td>NEFT</td><td>2025-11-29T00:00:00.000Z</td><td>Indiranagar</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50030</td><td>C1021</td><td>BR102</td><td>2225303.0</td><td>8.5</td><td>Education</td><td>12</td><td>CLOSED</td><td>2025-11-04</td><td>2025-11-13T00:00:00.000Z</td><td>Arjun Das</td><td>20-06-1989</td><td>SSSRC1170L</td><td>9.17737E+11</td><td>c1021@example.com</td><td>Hyderabad</td><td>1739312</td><td>2025-12-02T00:00:00.000Z</td><td>D06288607</td><td>2025-11-13</td><td>2225303.0</td><td>NEFT</td><td>2025-11-13T00:00:00.000Z</td><td>Powai</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50031</td><td>C1022</td><td>BR105</td><td>708111.0</td><td>12.5</td><td>Home</td><td>36</td><td>DISBURSED</td><td>2025-11-21</td><td>2025-11-25T00:00:00.000Z</td><td>Vihaan Patel</td><td>02-02-2024</td><td>PUHOF6540F</td><td>9.18657E+11</td><td>c1022@example.com</td><td>Hyderabad</td><td>1190407</td><td>2025-11-27T00:00:00.000Z</td><td>D00913128</td><td>2025-11-29</td><td>708111.0</td><td>IMPS</td><td>2025-11-25T00:00:00.000Z</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50032</td><td>C1023</td><td>BR105</td><td>2571542.0</td><td>8.5</td><td>Home</td><td>24</td><td>CLOSED</td><td>2025-11-28</td><td>2025-12-06T00:00:00.000Z</td><td>Meera Patel</td><td>25-03-1983</td><td>OEYAC9917F</td><td>9.17337E+11</td><td>null</td><td>Mumbai</td><td>301102</td><td>2025-11-29T00:00:00.000Z</td><td>D60504236</td><td>2025-12-08</td><td>2571542.0</td><td>RTGS</td><td>2025-12-06T00:00:00.000Z</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50033</td><td>C1023</td><td>BR104</td><td>2431087.0</td><td>9.2</td><td>Home</td><td>36</td><td>CLOSED</td><td>2025-11-09</td><td>2025-11-16T00:00:00.000Z</td><td>Meera Patel</td><td>25-03-1983</td><td>OEYAC9917F</td><td>9.17337E+11</td><td>null</td><td>Mumbai</td><td>301102</td><td>2025-11-29T00:00:00.000Z</td><td>D83025028</td><td>2025-11-11</td><td>2431087.0</td><td>RTGS</td><td>2025-11-20T00:00:00.000Z</td><td>Koramangala</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50034</td><td>C1024</td><td>BR106</td><td>2629669.0</td><td>9.2</td><td>Personal</td><td>36</td><td>CLOSED</td><td>2025-12-03</td><td>2025-12-04T00:00:00.000Z</td><td>Sara Sharma</td><td>03-05-2009</td><td>SGGTG7703H</td><td>9.17848E+11</td><td>c1024@example.com</td><td>Bengaluru</td><td>2276398</td><td>2025-12-12T00:00:00.000Z</td><td>D55094191</td><td>2025-12-09</td><td>2629669.0</td><td>IMPS</td><td>2025-12-07T00:00:00.000Z</td><td>Gachibowli</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50035</td><td>C1024</td><td>BR100</td><td>2794122.0</td><td>10.5</td><td>Personal</td><td>60</td><td>PENDING</td><td>2025-11-14</td><td>2025-11-22T00:00:00.000Z</td><td>Sara Sharma</td><td>03-05-2009</td><td>SGGTG7703H</td><td>9.17848E+11</td><td>c1024@example.com</td><td>Bengaluru</td><td>2276398</td><td>2025-12-12T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Andheri</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50036</td><td>C1025</td><td>BR104</td><td>1686846.0</td><td>9.2</td><td>Education</td><td>60</td><td>DISBURSED</td><td>2025-11-16</td><td>2025-11-23T00:00:00.000Z</td><td>Saanvi Nair</td><td>04-07-1982</td><td>MGTMM7855S</td><td>9.17989E+11</td><td>c1025@example.com</td><td>Kolkata</td><td>1816720</td><td>2025-11-30T00:00:00.000Z</td><td>D38172942</td><td>2025-11-17</td><td>1686846.0</td><td>NEFT</td><td>2025-11-24T00:00:00.000Z</td><td>Koramangala</td><td>Bengaluru</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50037</td><td>C1026</td><td>BR106</td><td>2885601.0</td><td>8.5</td><td>Home</td><td>36</td><td>APPROVED</td><td>2025-11-29</td><td>2025-12-04T00:00:00.000Z</td><td>Sara Iyer</td><td>07-10-1986</td><td>BQDKX6915P</td><td>9.18735E+11</td><td>c1026@example.com</td><td>Pune</td><td>1668135</td><td>2025-11-10T00:00:00.000Z</td><td>D41312749</td><td>2025-12-06</td><td>2164200.75</td><td>RTGS</td><td>2025-12-04T00:00:00.000Z</td><td>Gachibowli</td><td>Hyderabad</td><td>South</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50038</td><td>C1026</td><td>BR107</td><td>609285.0</td><td>9.2</td><td>Home</td><td>24</td><td>APPROVED</td><td>2025-11-13</td><td>2025-11-22T00:00:00.000Z</td><td>Sara Iyer</td><td>07-10-1986</td><td>BQDKX6915P</td><td>9.18735E+11</td><td>c1026@example.com</td><td>Pune</td><td>1668135</td><td>2025-11-10T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>SaltLake</td><td>Kolkata</td><td>East</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50039</td><td>C1027</td><td>BR102</td><td>2778717.0</td><td>8.5</td><td>Education</td><td>24</td><td>PENDING</td><td>2025-12-03</td><td>2025-12-08T00:00:00.000Z</td><td>Diya Singh</td><td>19-11-1991</td><td>CAQNW5333V</td><td>9.17356E+11</td><td>c1027@example.com</td><td>Hyderabad</td><td>2414269</td><td>2025-11-30T00:00:00.000Z</td><td>D19286445</td><td>2025-12-11</td><td>2778717.0</td><td>RTGS</td><td>2025-12-11T00:00:00.000Z</td><td>Powai</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50040</td><td>C1028</td><td>BR101</td><td>523372.0</td><td>11.0</td><td>Personal</td><td>12</td><td>APPROVED</td><td>2025-11-16</td><td>2025-11-17T00:00:00.000Z</td><td>Kabir Nair</td><td>17-07-1997</td><td>XIWGM1947B</td><td>9.17063E+11</td><td>c1028@example.com</td><td>Hyderabad</td><td>842930</td><td>2025-11-25T00:00:00.000Z</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>Bandra</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr><tr><td>L50041</td><td>C1029</td><td>BR100</td><td>349469.0</td><td>10.5</td><td>Education</td><td>48</td><td>DISBURSED</td><td>2025-11-30</td><td>2025-12-08T00:00:00.000Z</td><td>Kabir Khan</td><td>01-05-2010</td><td>EZBYY8839W</td><td>9.17797E+11</td><td>c1029@example.com</td><td>Pune</td><td>428320</td><td>2025-11-27T00:00:00.000Z</td><td>D81836337</td><td>2025-12-11</td><td>349469.0</td><td>IMPS</td><td>2025-12-09T00:00:00.000Z</td><td>Andheri</td><td>Mumbai</td><td>West</td><td>2025-12-24T07:44:48.741Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "L50001",
         "C1000",
         "BR105",
         2057234.0,
         10.5,
         "Personal",
         12,
         "APPROVED",
         "2025-11-26",
         "2025-12-01T00:00:00.000Z",
         null,
         "05-04-2007",
         "JLBIB2876M",
         null,
         "c1000@example.com",
         "Chennai",
         "443377",
         "2025-11-15T00:00:00.000Z",
         "D84436419",
         "2025-12-01",
         2057234.0,
         "RTGS",
         "2025-12-04T00:00:00.000Z",
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50002",
         "C1001",
         "BR106",
         1960486.0,
         8.5,
         "Auto",
         48,
         "PENDING",
         "2025-11-23",
         "2025-11-23T00:00:00.000Z",
         "Kabir Singh",
         "23-08-1979",
         "NAWIQ1462F",
         "9.18106E+11",
         "c1001@example.com",
         "Kolkata",
         "2445150",
         "2025-11-24T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "Gachibowli",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50003",
         "C1002",
         "BR106",
         727030.0,
         9.2,
         "Auto",
         48,
         "PENDING",
         "2025-11-18",
         "2025-11-19T00:00:00.000Z",
         "Meera Nair",
         "17-04-2000",
         "GBGRS7166F",
         "9.16512E+11",
         "c1002@example.com",
         "Mumbai",
         "1375791",
         "2025-12-11T00:00:00.000Z",
         "D37499621",
         "2025-12-01",
         654327.0,
         "IMPS",
         "2025-11-20T00:00:00.000Z",
         "Gachibowli",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50004",
         "C1003",
         "BR103",
         2421753.0,
         8.5,
         "Home",
         60,
         "PENDING",
         "2025-11-21",
         "2025-11-26T00:00:00.000Z",
         "Vihaan Khan",
         "02-09-2020",
         "WTMHM5219V",
         "9.19126E+11",
         "c1003@example.com",
         "Mumbai",
         "1698696",
         "2025-11-19T00:00:00.000Z",
         "D64403047",
         "2025-11-30",
         2421753.0,
         "IMPS",
         "2025-11-28T00:00:00.000Z",
         "Indiranagar",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50005",
         "C1004",
         "BR104",
         1914865.0,
         10.5,
         "Auto",
         12,
         "PENDING",
         "2025-11-13",
         "2025-11-22T00:00:00.000Z",
         "Ishaan Singh",
         "14-03-1997",
         "HFXEI9849G",
         "9.16909E+11",
         "c1004@example.com",
         "Chennai",
         "2247752",
         "2025-11-23T00:00:00.000Z",
         "D96808887",
         "2025-11-13",
         1914865.0,
         "NEFT",
         "2025-11-23T00:00:00.000Z",
         "Koramangala",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50006",
         "C1005",
         "BR105",
         1507901.0,
         11.0,
         "Personal",
         24,
         "PENDING",
         "2025-11-17",
         "2025-11-21T00:00:00.000Z",
         "Aarav Singh",
         "15-10-2015",
         "EBTUK4244M",
         "9.18527E+11",
         "c1005@example.com",
         "Hyderabad",
         "1826176",
         "2025-11-14T00:00:00.000Z",
         "D29721737",
         "2025-11-18",
         1357110.9,
         "RTGS",
         "2025-11-23T00:00:00.000Z",
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50007",
         "C1006",
         "BR101",
         1435270.0,
         12.5,
         "Home",
         36,
         "APPROVED",
         "2025-11-18",
         "2025-11-22T00:00:00.000Z",
         "Ananya Gupta",
         "02-11-2022",
         "OSSRD4960D",
         "9.17783E+11",
         "c1006@example.com",
         "Kolkata",
         "2259522",
         "2025-11-12T00:00:00.000Z",
         "D10346384",
         "2025-12-01",
         1435270.0,
         "RTGS",
         "2025-11-26T00:00:00.000Z",
         "Bandra",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50008",
         "C1006",
         "BR100",
         947531.0,
         10.5,
         "Personal",
         36,
         "CLOSED",
         "2025-11-29",
         "2025-12-07T00:00:00.000Z",
         "Ananya Gupta",
         "02-11-2022",
         "OSSRD4960D",
         "9.17783E+11",
         "c1006@example.com",
         "Kolkata",
         "2259522",
         "2025-11-12T00:00:00.000Z",
         "D47948592",
         "2025-12-12",
         947531.0,
         "IMPS",
         "2025-12-08T00:00:00.000Z",
         "Andheri",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50009",
         "C1007",
         "BR102",
         260513.0,
         8.5,
         "Auto",
         60,
         "APPROVED",
         "2025-11-10",
         "2025-11-15T00:00:00.000Z",
         "Riya Singh",
         "07-12-1992",
         "UKVEH0439T",
         "9.17867E+11",
         null,
         "Hyderabad",
         "1479781",
         "2025-11-25T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "Powai",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50010",
         "C1008",
         "BR105",
         1751301.0,
         11.0,
         "Education",
         48,
         "PENDING",
         "2025-11-11",
         "2025-11-12T00:00:00.000Z",
         "Ananya Das",
         "29-08-2008",
         "MMBHD7525N",
         "9.1969E+11",
         "c1008@example.com",
         "Pune",
         "1925714",
         "2025-11-16T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50011",
         "C1009",
         "BR104",
         2046383.0,
         12.5,
         "Auto",
         48,
         "PENDING",
         "2025-11-09",
         "2025-11-12T00:00:00.000Z",
         "Ananya Das",
         "17-08-2019",
         "JBNFR1197O",
         "9.17482E+11",
         "c1009@example.com",
         "Chennai",
         "2040279",
         "2025-11-17T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "Koramangala",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50012",
         "C1010",
         "BR103",
         2764833.0,
         9.2,
         "Education",
         12,
         "APPROVED",
         "2025-11-22",
         "2025-11-23T00:00:00.000Z",
         "Saanvi Gupta",
         "27-05-1981",
         "PQCQL0958W",
         "9.18746E+11",
         "c1010@example.com",
         "Kolkata",
         "1320336",
         "2025-11-12T00:00:00.000Z",
         "D02837874",
         "2025-12-04",
         2764833.0,
         "RTGS",
         "2025-11-24T00:00:00.000Z",
         "Indiranagar",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50013",
         "C1011",
         "BR107",
         1238582.0,
         10.5,
         "Home",
         60,
         "APPROVED",
         "2025-11-20",
         "2025-11-29T00:00:00.000Z",
         "Aarav Iyer",
         "04-05-2021",
         "TJXGC5407J",
         "9.17749E+11",
         "c1011@example.com",
         "Mumbai",
         "1666969",
         "2025-12-07T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "SaltLake",
         "Kolkata",
         "East",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50014",
         "C1011",
         "BR103",
         767667.0,
         12.5,
         "Home",
         48,
         "PENDING",
         "2025-11-12",
         "2025-11-17T00:00:00.000Z",
         "Aarav Iyer",
         "04-05-2021",
         "TJXGC5407J",
         "9.17749E+11",
         "c1011@example.com",
         "Mumbai",
         "1666969",
         "2025-12-07T00:00:00.000Z",
         "D59846102",
         "2025-11-19",
         767667.0,
         "IMPS",
         "2025-11-20T00:00:00.000Z",
         "Indiranagar",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50015",
         "C1012",
         "BR105",
         2975871.0,
         12.5,
         "Personal",
         36,
         "DISBURSED",
         "2025-11-09",
         "2025-11-09T00:00:00.000Z",
         "Aditya Gupta",
         "24-05-1990",
         "QGGRD5607D",
         "9.17741E+11",
         "c1012@example.com",
         "Bengaluru",
         "2060719",
         "2025-11-23T00:00:00.000Z",
         "D53273166",
         "2025-11-18",
         2975871.0,
         "RTGS",
         "2025-11-09T00:00:00.000Z",
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50016",
         "C1012",
         "BR105",
         1418271.0,
         8.5,
         "Education",
         60,
         "CLOSED",
         "2025-11-11",
         "2025-11-11T00:00:00.000Z",
         "Aditya Gupta",
         "24-05-1990",
         "QGGRD5607D",
         "9.17741E+11",
         "c1012@example.com",
         "Bengaluru",
         "2060719",
         "2025-11-23T00:00:00.000Z",
         "D43719185",
         "2025-11-13",
         1418271.0,
         "RTGS",
         "2025-11-11T00:00:00.000Z",
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50017",
         "C1013",
         "BR105",
         2694218.0,
         8.5,
         "Home",
         36,
         "DISBURSED",
         "2025-11-30",
         "2025-12-04T00:00:00.000Z",
         "Sara Patel",
         "22-10-1986",
         "RKUNC0665I",
         "9.1873E+11",
         "c1013@mail.com",
         "Hyderabad",
         "2123657",
         "2025-12-07T00:00:00.000Z",
         "D65074561",
         "2025-12-06",
         2694218.0,
         "IMPS",
         "2025-12-05T00:00:00.000Z",
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50018",
         "C1013",
         "BR103",
         261919.0,
         11.0,
         "Education",
         36,
         "CLOSED",
         "2025-11-12",
         "2025-11-15T00:00:00.000Z",
         "Sara Patel",
         "22-10-1986",
         "RKUNC0665I",
         "9.1873E+11",
         "c1013@mail.com",
         "Hyderabad",
         "2123657",
         "2025-12-07T00:00:00.000Z",
         "D23083211",
         "2025-11-17",
         261919.0,
         "IMPS",
         "2025-11-18T00:00:00.000Z",
         "Indiranagar",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50019",
         "C1014",
         "BR104",
         2262528.0,
         11.0,
         "Auto",
         12,
         "PENDING",
         "2025-11-04",
         "2025-11-13T00:00:00.000Z",
         "Meera Reddy",
         "13-05-1975",
         "VQQNJ1765G",
         "9.19187E+11",
         "c1014@example.com",
         "Mumbai",
         "2020579",
         "2025-11-18T00:00:00.000Z",
         "D89490099",
         "2025-11-08",
         2262528.0,
         "RTGS",
         "2025-11-14T00:00:00.000Z",
         "Koramangala",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50020",
         "C1014",
         "BR102",
         874717.0,
         8.5,
         "Personal",
         60,
         "CLOSED",
         "2025-12-04",
         "2025-12-12T00:00:00.000Z",
         "Meera Reddy",
         "13-05-1975",
         "VQQNJ1765G",
         "9.19187E+11",
         "c1014@example.com",
         "Mumbai",
         "2020579",
         "2025-11-18T00:00:00.000Z",
         "D57524236",
         "2025-12-08",
         874717.0,
         "IMPS",
         "2025-12-15T00:00:00.000Z",
         "Powai",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50021",
         "C1015",
         "BR100",
         701951.0,
         11.0,
         "Home",
         60,
         "CLOSED",
         "2025-11-27",
         "2025-11-27T00:00:00.000Z",
         "Arjun Khan",
         "28-08-1988",
         "NLFGZ8057J",
         "9.18223E+11",
         "c1015@example.com",
         "Hyderabad",
         "1282560",
         "2025-11-25T00:00:00.000Z",
         "D15832328",
         "2025-12-10",
         701951.0,
         "RTGS",
         "2025-11-30T00:00:00.000Z",
         "Andheri",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50022",
         "C1016",
         "BR104",
         1068906.0,
         9.2,
         "Personal",
         60,
         "APPROVED",
         "2025-11-10",
         "2025-11-14T00:00:00.000Z",
         "Ishaan Sharma",
         "05-02-1987",
         "COBXT5630Q",
         "9.16458E+11",
         "c1016@example.com",
         "Chennai",
         "1722464",
         "2025-11-04T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "Koramangala",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50023",
         "C1017",
         "BR105",
         602913.0,
         8.5,
         "Personal",
         24,
         "APPROVED",
         "2025-11-16",
         "2025-11-18T00:00:00.000Z",
         "Saanvi Khan",
         "09-11-2002",
         "SLJKD7615W",
         "9.16317E+11",
         "c1017@example.com",
         "Mumbai",
         "1971247",
         "2025-11-27T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50024",
         "C1017",
         "BR107",
         250291.0,
         11.0,
         "Personal",
         24,
         "PENDING",
         "2025-11-05",
         "2025-11-10T00:00:00.000Z",
         "Saanvi Khan",
         "09-11-2002",
         "SLJKD7615W",
         "9.16317E+11",
         "c1017@example.com",
         "Mumbai",
         "1971247",
         "2025-11-27T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "SaltLake",
         "Kolkata",
         "East",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50025",
         "C1018",
         "BR105",
         2345326.0,
         10.5,
         "Auto",
         60,
         "CLOSED",
         "2025-11-25",
         "2025-12-03T00:00:00.000Z",
         "Meera Das",
         "19-06-1979",
         "SOSQF9918D",
         "9.18237E+11",
         "c1018@example.com",
         "Chennai",
         "1606386",
         "2025-11-25T00:00:00.000Z",
         "D19658626",
         "2025-11-29",
         2345326.0,
         "NEFT",
         "2025-12-05T00:00:00.000Z",
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50026",
         "C1018",
         "BR102",
         2203139.0,
         12.5,
         "Personal",
         12,
         "DISBURSED",
         "2025-11-01",
         "2025-11-05T00:00:00.000Z",
         "Meera Das",
         "19-06-1979",
         "SOSQF9918D",
         "9.18237E+11",
         "c1018@example.com",
         "Chennai",
         "1606386",
         "2025-11-25T00:00:00.000Z",
         "D60486181",
         "2025-11-08",
         2203139.0,
         "RTGS",
         "2025-11-05T00:00:00.000Z",
         "Powai",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50027",
         "C1019",
         "BR104",
         2797175.0,
         12.5,
         "Auto",
         24,
         "PENDING",
         "2025-11-20",
         "2025-11-21T00:00:00.000Z",
         "Aditya Patel",
         "09-07-1996",
         "WTHUB5577E",
         "9.19931E+11",
         "c1019@example.com",
         "Chennai",
         "732815",
         "2025-11-06T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "Koramangala",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50028",
         "C1020",
         "BR102",
         2056019.0,
         10.5,
         "Education",
         24,
         "APPROVED",
         "2025-11-18",
         "2025-11-27T00:00:00.000Z",
         "Kabir Nair",
         "16-09-1976",
         "LEQPU9531T",
         "9.16936E+11",
         null,
         "Kolkata",
         "2175259",
         "2025-12-12T00:00:00.000Z",
         "D63914701",
         "2025-11-22",
         2056019.0,
         "IMPS",
         "2025-11-29T00:00:00.000Z",
         "Powai",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50029",
         "C1021",
         "BR103",
         269606.0,
         8.5,
         "Auto",
         48,
         "CLOSED",
         "2025-11-18",
         "2025-11-26T00:00:00.000Z",
         "Arjun Das",
         "20-06-1989",
         "SSSRC1170L",
         "9.17737E+11",
         "c1021@example.com",
         "Hyderabad",
         "1739312",
         "2025-12-02T00:00:00.000Z",
         "D15592542",
         "2025-11-30",
         242645.4,
         "NEFT",
         "2025-11-29T00:00:00.000Z",
         "Indiranagar",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50030",
         "C1021",
         "BR102",
         2225303.0,
         8.5,
         "Education",
         12,
         "CLOSED",
         "2025-11-04",
         "2025-11-13T00:00:00.000Z",
         "Arjun Das",
         "20-06-1989",
         "SSSRC1170L",
         "9.17737E+11",
         "c1021@example.com",
         "Hyderabad",
         "1739312",
         "2025-12-02T00:00:00.000Z",
         "D06288607",
         "2025-11-13",
         2225303.0,
         "NEFT",
         "2025-11-13T00:00:00.000Z",
         "Powai",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50031",
         "C1022",
         "BR105",
         708111.0,
         12.5,
         "Home",
         36,
         "DISBURSED",
         "2025-11-21",
         "2025-11-25T00:00:00.000Z",
         "Vihaan Patel",
         "02-02-2024",
         "PUHOF6540F",
         "9.18657E+11",
         "c1022@example.com",
         "Hyderabad",
         "1190407",
         "2025-11-27T00:00:00.000Z",
         "D00913128",
         "2025-11-29",
         708111.0,
         "IMPS",
         "2025-11-25T00:00:00.000Z",
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50032",
         "C1023",
         "BR105",
         2571542.0,
         8.5,
         "Home",
         24,
         "CLOSED",
         "2025-11-28",
         "2025-12-06T00:00:00.000Z",
         "Meera Patel",
         "25-03-1983",
         "OEYAC9917F",
         "9.17337E+11",
         null,
         "Mumbai",
         "301102",
         "2025-11-29T00:00:00.000Z",
         "D60504236",
         "2025-12-08",
         2571542.0,
         "RTGS",
         "2025-12-06T00:00:00.000Z",
         "HitechCity",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50033",
         "C1023",
         "BR104",
         2431087.0,
         9.2,
         "Home",
         36,
         "CLOSED",
         "2025-11-09",
         "2025-11-16T00:00:00.000Z",
         "Meera Patel",
         "25-03-1983",
         "OEYAC9917F",
         "9.17337E+11",
         null,
         "Mumbai",
         "301102",
         "2025-11-29T00:00:00.000Z",
         "D83025028",
         "2025-11-11",
         2431087.0,
         "RTGS",
         "2025-11-20T00:00:00.000Z",
         "Koramangala",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50034",
         "C1024",
         "BR106",
         2629669.0,
         9.2,
         "Personal",
         36,
         "CLOSED",
         "2025-12-03",
         "2025-12-04T00:00:00.000Z",
         "Sara Sharma",
         "03-05-2009",
         "SGGTG7703H",
         "9.17848E+11",
         "c1024@example.com",
         "Bengaluru",
         "2276398",
         "2025-12-12T00:00:00.000Z",
         "D55094191",
         "2025-12-09",
         2629669.0,
         "IMPS",
         "2025-12-07T00:00:00.000Z",
         "Gachibowli",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50035",
         "C1024",
         "BR100",
         2794122.0,
         10.5,
         "Personal",
         60,
         "PENDING",
         "2025-11-14",
         "2025-11-22T00:00:00.000Z",
         "Sara Sharma",
         "03-05-2009",
         "SGGTG7703H",
         "9.17848E+11",
         "c1024@example.com",
         "Bengaluru",
         "2276398",
         "2025-12-12T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "Andheri",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50036",
         "C1025",
         "BR104",
         1686846.0,
         9.2,
         "Education",
         60,
         "DISBURSED",
         "2025-11-16",
         "2025-11-23T00:00:00.000Z",
         "Saanvi Nair",
         "04-07-1982",
         "MGTMM7855S",
         "9.17989E+11",
         "c1025@example.com",
         "Kolkata",
         "1816720",
         "2025-11-30T00:00:00.000Z",
         "D38172942",
         "2025-11-17",
         1686846.0,
         "NEFT",
         "2025-11-24T00:00:00.000Z",
         "Koramangala",
         "Bengaluru",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50037",
         "C1026",
         "BR106",
         2885601.0,
         8.5,
         "Home",
         36,
         "APPROVED",
         "2025-11-29",
         "2025-12-04T00:00:00.000Z",
         "Sara Iyer",
         "07-10-1986",
         "BQDKX6915P",
         "9.18735E+11",
         "c1026@example.com",
         "Pune",
         "1668135",
         "2025-11-10T00:00:00.000Z",
         "D41312749",
         "2025-12-06",
         2164200.75,
         "RTGS",
         "2025-12-04T00:00:00.000Z",
         "Gachibowli",
         "Hyderabad",
         "South",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50038",
         "C1026",
         "BR107",
         609285.0,
         9.2,
         "Home",
         24,
         "APPROVED",
         "2025-11-13",
         "2025-11-22T00:00:00.000Z",
         "Sara Iyer",
         "07-10-1986",
         "BQDKX6915P",
         "9.18735E+11",
         "c1026@example.com",
         "Pune",
         "1668135",
         "2025-11-10T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "SaltLake",
         "Kolkata",
         "East",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50039",
         "C1027",
         "BR102",
         2778717.0,
         8.5,
         "Education",
         24,
         "PENDING",
         "2025-12-03",
         "2025-12-08T00:00:00.000Z",
         "Diya Singh",
         "19-11-1991",
         "CAQNW5333V",
         "9.17356E+11",
         "c1027@example.com",
         "Hyderabad",
         "2414269",
         "2025-11-30T00:00:00.000Z",
         "D19286445",
         "2025-12-11",
         2778717.0,
         "RTGS",
         "2025-12-11T00:00:00.000Z",
         "Powai",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50040",
         "C1028",
         "BR101",
         523372.0,
         11.0,
         "Personal",
         12,
         "APPROVED",
         "2025-11-16",
         "2025-11-17T00:00:00.000Z",
         "Kabir Nair",
         "17-07-1997",
         "XIWGM1947B",
         "9.17063E+11",
         "c1028@example.com",
         "Hyderabad",
         "842930",
         "2025-11-25T00:00:00.000Z",
         null,
         null,
         null,
         null,
         null,
         "Bandra",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ],
        [
         "L50041",
         "C1029",
         "BR100",
         349469.0,
         10.5,
         "Education",
         48,
         "DISBURSED",
         "2025-11-30",
         "2025-12-08T00:00:00.000Z",
         "Kabir Khan",
         "01-05-2010",
         "EZBYY8839W",
         "9.17797E+11",
         "c1029@example.com",
         "Pune",
         "428320",
         "2025-11-27T00:00:00.000Z",
         "D81836337",
         "2025-12-11",
         349469.0,
         "IMPS",
         "2025-12-09T00:00:00.000Z",
         "Andheri",
         "Mumbai",
         "West",
         "2025-12-24T07:44:48.741Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "loan_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "branch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loan_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "interest_rate",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "loan_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tenure_months",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loan_origination_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loan_update_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "full_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pan",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "mobile",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "annual_income",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_update_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "disbursement_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "disbursement_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "disbursement_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "payment_mode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "disbursement_update_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "branch_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "branch_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "silver_ingest_ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gold_base.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24297ec2-1bcd-4a58-9039-93a717cd3ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#customer exposure mart\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "gold_customer_360 = (\n",
    "    gold_base\n",
    "    .groupBy(\n",
    "        \"customer_id\",\n",
    "        \"full_name\",\n",
    "        \"customer_city\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.countDistinct(\"loan_id\").alias(\"total_loans\"),\n",
    "        F.sum(\"loan_amount\").alias(\"total_loan_exposure\"),\n",
    "        F.avg(\"loan_amount\").alias(\"avg_loan_amount\"),\n",
    "        F.max(\"loan_amount\").alias(\"max_loan_amount\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2daea79-3fe5-4723-8ef7-256aa3dcdcb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_customer_360.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"gold_customer_360\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2611e9-c4af-4c8b-b427-6fca74d41734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>full_name</th><th>customer_city</th><th>total_loans</th><th>total_loan_exposure</th><th>avg_loan_amount</th><th>max_loan_amount</th></tr></thead><tbody><tr><td>C1022</td><td>Vihaan Patel</td><td>Hyderabad</td><td>1</td><td>708111.0</td><td>708111.0</td><td>708111.0</td></tr><tr><td>C1005</td><td>Aarav Singh</td><td>Hyderabad</td><td>1</td><td>1507901.0</td><td>1507901.0</td><td>1507901.0</td></tr><tr><td>C1021</td><td>Arjun Das</td><td>Hyderabad</td><td>2</td><td>2494909.0</td><td>1247454.5</td><td>2225303.0</td></tr><tr><td>C1015</td><td>Arjun Khan</td><td>Hyderabad</td><td>1</td><td>701951.0</td><td>701951.0</td><td>701951.0</td></tr><tr><td>C1019</td><td>Aditya Patel</td><td>Chennai</td><td>1</td><td>2797175.0</td><td>2797175.0</td><td>2797175.0</td></tr><tr><td>C1023</td><td>Meera Patel</td><td>Mumbai</td><td>2</td><td>5002629.0</td><td>2501314.5</td><td>2571542.0</td></tr><tr><td>C1006</td><td>Ananya Gupta</td><td>Kolkata</td><td>2</td><td>2382801.0</td><td>1191400.5</td><td>1435270.0</td></tr><tr><td>C1020</td><td>Kabir Nair</td><td>Kolkata</td><td>1</td><td>2056019.0</td><td>2056019.0</td><td>2056019.0</td></tr><tr><td>C1011</td><td>Aarav Iyer</td><td>Mumbai</td><td>2</td><td>2006249.0</td><td>1003124.5</td><td>1238582.0</td></tr><tr><td>C1001</td><td>Kabir Singh</td><td>Kolkata</td><td>1</td><td>1960486.0</td><td>1960486.0</td><td>1960486.0</td></tr><tr><td>C1004</td><td>Ishaan Singh</td><td>Chennai</td><td>1</td><td>1914865.0</td><td>1914865.0</td><td>1914865.0</td></tr><tr><td>C1003</td><td>Vihaan Khan</td><td>Mumbai</td><td>1</td><td>2421753.0</td><td>2421753.0</td><td>2421753.0</td></tr><tr><td>C1007</td><td>Riya Singh</td><td>Hyderabad</td><td>1</td><td>260513.0</td><td>260513.0</td><td>260513.0</td></tr><tr><td>C1016</td><td>Ishaan Sharma</td><td>Chennai</td><td>1</td><td>1068906.0</td><td>1068906.0</td><td>1068906.0</td></tr><tr><td>C1026</td><td>Sara Iyer</td><td>Pune</td><td>2</td><td>3494886.0</td><td>1747443.0</td><td>2885601.0</td></tr><tr><td>C1000</td><td>null</td><td>Chennai</td><td>1</td><td>2057234.0</td><td>2057234.0</td><td>2057234.0</td></tr><tr><td>C1012</td><td>Aditya Gupta</td><td>Bengaluru</td><td>2</td><td>4394142.0</td><td>2197071.0</td><td>2975871.0</td></tr><tr><td>C1028</td><td>Kabir Nair</td><td>Hyderabad</td><td>1</td><td>523372.0</td><td>523372.0</td><td>523372.0</td></tr><tr><td>C1027</td><td>Diya Singh</td><td>Hyderabad</td><td>1</td><td>2778717.0</td><td>2778717.0</td><td>2778717.0</td></tr><tr><td>C1009</td><td>Ananya Das</td><td>Chennai</td><td>1</td><td>2046383.0</td><td>2046383.0</td><td>2046383.0</td></tr><tr><td>C1029</td><td>Kabir Khan</td><td>Pune</td><td>1</td><td>349469.0</td><td>349469.0</td><td>349469.0</td></tr><tr><td>C1002</td><td>Meera Nair</td><td>Mumbai</td><td>1</td><td>727030.0</td><td>727030.0</td><td>727030.0</td></tr><tr><td>C1010</td><td>Saanvi Gupta</td><td>Kolkata</td><td>1</td><td>2764833.0</td><td>2764833.0</td><td>2764833.0</td></tr><tr><td>C1025</td><td>Saanvi Nair</td><td>Kolkata</td><td>1</td><td>1686846.0</td><td>1686846.0</td><td>1686846.0</td></tr><tr><td>C1017</td><td>Saanvi Khan</td><td>Mumbai</td><td>2</td><td>853204.0</td><td>426602.0</td><td>602913.0</td></tr><tr><td>C1014</td><td>Meera Reddy</td><td>Mumbai</td><td>2</td><td>3137245.0</td><td>1568622.5</td><td>2262528.0</td></tr><tr><td>C1008</td><td>Ananya Das</td><td>Pune</td><td>1</td><td>1751301.0</td><td>1751301.0</td><td>1751301.0</td></tr><tr><td>C1024</td><td>Sara Sharma</td><td>Bengaluru</td><td>2</td><td>5423791.0</td><td>2711895.5</td><td>2794122.0</td></tr><tr><td>C1018</td><td>Meera Das</td><td>Chennai</td><td>2</td><td>4548465.0</td><td>2274232.5</td><td>2345326.0</td></tr><tr><td>C1013</td><td>Sara Patel</td><td>Hyderabad</td><td>2</td><td>2956137.0</td><td>1478068.5</td><td>2694218.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "C1022",
         "Vihaan Patel",
         "Hyderabad",
         1,
         708111.0,
         708111.0,
         708111.0
        ],
        [
         "C1005",
         "Aarav Singh",
         "Hyderabad",
         1,
         1507901.0,
         1507901.0,
         1507901.0
        ],
        [
         "C1021",
         "Arjun Das",
         "Hyderabad",
         2,
         2494909.0,
         1247454.5,
         2225303.0
        ],
        [
         "C1015",
         "Arjun Khan",
         "Hyderabad",
         1,
         701951.0,
         701951.0,
         701951.0
        ],
        [
         "C1019",
         "Aditya Patel",
         "Chennai",
         1,
         2797175.0,
         2797175.0,
         2797175.0
        ],
        [
         "C1023",
         "Meera Patel",
         "Mumbai",
         2,
         5002629.0,
         2501314.5,
         2571542.0
        ],
        [
         "C1006",
         "Ananya Gupta",
         "Kolkata",
         2,
         2382801.0,
         1191400.5,
         1435270.0
        ],
        [
         "C1020",
         "Kabir Nair",
         "Kolkata",
         1,
         2056019.0,
         2056019.0,
         2056019.0
        ],
        [
         "C1011",
         "Aarav Iyer",
         "Mumbai",
         2,
         2006249.0,
         1003124.5,
         1238582.0
        ],
        [
         "C1001",
         "Kabir Singh",
         "Kolkata",
         1,
         1960486.0,
         1960486.0,
         1960486.0
        ],
        [
         "C1004",
         "Ishaan Singh",
         "Chennai",
         1,
         1914865.0,
         1914865.0,
         1914865.0
        ],
        [
         "C1003",
         "Vihaan Khan",
         "Mumbai",
         1,
         2421753.0,
         2421753.0,
         2421753.0
        ],
        [
         "C1007",
         "Riya Singh",
         "Hyderabad",
         1,
         260513.0,
         260513.0,
         260513.0
        ],
        [
         "C1016",
         "Ishaan Sharma",
         "Chennai",
         1,
         1068906.0,
         1068906.0,
         1068906.0
        ],
        [
         "C1026",
         "Sara Iyer",
         "Pune",
         2,
         3494886.0,
         1747443.0,
         2885601.0
        ],
        [
         "C1000",
         null,
         "Chennai",
         1,
         2057234.0,
         2057234.0,
         2057234.0
        ],
        [
         "C1012",
         "Aditya Gupta",
         "Bengaluru",
         2,
         4394142.0,
         2197071.0,
         2975871.0
        ],
        [
         "C1028",
         "Kabir Nair",
         "Hyderabad",
         1,
         523372.0,
         523372.0,
         523372.0
        ],
        [
         "C1027",
         "Diya Singh",
         "Hyderabad",
         1,
         2778717.0,
         2778717.0,
         2778717.0
        ],
        [
         "C1009",
         "Ananya Das",
         "Chennai",
         1,
         2046383.0,
         2046383.0,
         2046383.0
        ],
        [
         "C1029",
         "Kabir Khan",
         "Pune",
         1,
         349469.0,
         349469.0,
         349469.0
        ],
        [
         "C1002",
         "Meera Nair",
         "Mumbai",
         1,
         727030.0,
         727030.0,
         727030.0
        ],
        [
         "C1010",
         "Saanvi Gupta",
         "Kolkata",
         1,
         2764833.0,
         2764833.0,
         2764833.0
        ],
        [
         "C1025",
         "Saanvi Nair",
         "Kolkata",
         1,
         1686846.0,
         1686846.0,
         1686846.0
        ],
        [
         "C1017",
         "Saanvi Khan",
         "Mumbai",
         2,
         853204.0,
         426602.0,
         602913.0
        ],
        [
         "C1014",
         "Meera Reddy",
         "Mumbai",
         2,
         3137245.0,
         1568622.5,
         2262528.0
        ],
        [
         "C1008",
         "Ananya Das",
         "Pune",
         1,
         1751301.0,
         1751301.0,
         1751301.0
        ],
        [
         "C1024",
         "Sara Sharma",
         "Bengaluru",
         2,
         5423791.0,
         2711895.5,
         2794122.0
        ],
        [
         "C1018",
         "Meera Das",
         "Chennai",
         2,
         4548465.0,
         2274232.5,
         2345326.0
        ],
        [
         "C1013",
         "Sara Patel",
         "Hyderabad",
         2,
         2956137.0,
         1478068.5,
         2694218.0
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "customer_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "full_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "customer_city",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_loans",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_loan_exposure",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "avg_loan_amount",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "max_loan_amount",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 53
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "full_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_loans",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_loan_exposure",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_loan_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "max_loan_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from gold_customer_360;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b370002-ddb9-49a0-8ab5-2252adeeb724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#branch Performance\n",
    "gold_branch_performance = (\n",
    "    gold_base\n",
    "    .groupBy(\n",
    "        \"branch_id\",\n",
    "        \"branch_name\",\n",
    "        \"branch_city\",\n",
    "        \"region\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.countDistinct(\"loan_id\").alias(\"loan_count\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "        F.sum(\"loan_amount\").alias(\"total_branch_exposure\"),\n",
    "        F.avg(\"loan_amount\").alias(\"avg_loan_amount\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c98757-acad-4990-a58e-3af4b595fa7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_branch_performance.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"gold_branch_performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a47ba5-c123-4be9-872e-467f2823326e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>branch_id</th><th>branch_name</th><th>branch_city</th><th>region</th><th>loan_count</th><th>customer_count</th><th>total_branch_exposure</th><th>avg_loan_amount</th></tr></thead><tbody><tr><td>BR105</td><td>HitechCity</td><td>Hyderabad</td><td>South</td><td>10</td><td>9</td><td>1.8632688E7</td><td>1863268.8</td></tr><tr><td>BR101</td><td>Bandra</td><td>Mumbai</td><td>West</td><td>2</td><td>2</td><td>1958642.0</td><td>979321.0</td></tr><tr><td>BR107</td><td>SaltLake</td><td>Kolkata</td><td>East</td><td>3</td><td>3</td><td>2098158.0</td><td>699386.0</td></tr><tr><td>BR102</td><td>Powai</td><td>Mumbai</td><td>West</td><td>6</td><td>6</td><td>1.0398408E7</td><td>1733068.0</td></tr><tr><td>BR100</td><td>Andheri</td><td>Mumbai</td><td>West</td><td>4</td><td>4</td><td>4793073.0</td><td>1198268.25</td></tr><tr><td>BR104</td><td>Koramangala</td><td>Bengaluru</td><td>South</td><td>7</td><td>7</td><td>1.420779E7</td><td>2029684.2857142857</td></tr><tr><td>BR103</td><td>Indiranagar</td><td>Bengaluru</td><td>South</td><td>5</td><td>5</td><td>6485778.0</td><td>1297155.6</td></tr><tr><td>BR106</td><td>Gachibowli</td><td>Hyderabad</td><td>South</td><td>4</td><td>4</td><td>8202786.0</td><td>2050696.5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "BR105",
         "HitechCity",
         "Hyderabad",
         "South",
         10,
         9,
         1.8632688E7,
         1863268.8
        ],
        [
         "BR101",
         "Bandra",
         "Mumbai",
         "West",
         2,
         2,
         1958642.0,
         979321.0
        ],
        [
         "BR107",
         "SaltLake",
         "Kolkata",
         "East",
         3,
         3,
         2098158.0,
         699386.0
        ],
        [
         "BR102",
         "Powai",
         "Mumbai",
         "West",
         6,
         6,
         1.0398408E7,
         1733068.0
        ],
        [
         "BR100",
         "Andheri",
         "Mumbai",
         "West",
         4,
         4,
         4793073.0,
         1198268.25
        ],
        [
         "BR104",
         "Koramangala",
         "Bengaluru",
         "South",
         7,
         7,
         1.420779E7,
         2029684.2857142857
        ],
        [
         "BR103",
         "Indiranagar",
         "Bengaluru",
         "South",
         5,
         5,
         6485778.0,
         1297155.6
        ],
        [
         "BR106",
         "Gachibowli",
         "Hyderabad",
         "South",
         4,
         4,
         8202786.0,
         2050696.5
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "branch_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "branch_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "branch_city",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "region",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "loan_count",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "customer_count",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_branch_exposure",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "avg_loan_amount",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 57
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "branch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "branch_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "branch_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loan_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_branch_exposure",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_loan_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from gold_branch_performance;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d89c2f-a611-4f59-8062-7b739b94f34f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_loan_lifecycle = (\n",
    "    gold_base\n",
    "    .groupBy(\"status\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"loan_id\").alias(\"loan_count\"),\n",
    "        F.sum(\"loan_amount\").alias(\"total_amount\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14efe4be-0e2a-4bc6-9db0-531f54e965de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_loan_lifecycle.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"gold_loan_lifecycle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbb3c3a-19e3-422b-a1de-069630a2367b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>status</th><th>loan_count</th><th>total_amount</th></tr></thead><tbody><tr><td>CLOSED</td><td>11</td><td>1.6676922E7</td></tr><tr><td>DISBURSED</td><td>6</td><td>1.0617654E7</td></tr><tr><td>APPROVED</td><td>11</td><td>1.5502528E7</td></tr><tr><td>PENDING</td><td>13</td><td>2.3980219E7</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CLOSED",
         11,
         1.6676922E7
        ],
        [
         "DISBURSED",
         6,
         1.0617654E7
        ],
        [
         "APPROVED",
         11,
         1.5502528E7
        ],
        [
         "PENDING",
         13,
         2.3980219E7
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "loan_count",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_amount",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 59
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loan_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from gold_loan_lifecycle;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d3ba11-cd25-455b-bd12-4d84cf475098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_disbursement_efficiency = (\n",
    "    gold_base\n",
    "    .withColumn(\n",
    "        \"disbursement_delay_days\",\n",
    "        F.datediff(\"disbursement_date\", \"loan_origination_date\")\n",
    "    )\n",
    "    .groupBy(\n",
    "        \"branch_id\",\n",
    "        \"branch_name\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.avg(\"disbursement_delay_days\").alias(\"avg_disbursement_delay_days\"),\n",
    "        F.count(\"loan_id\").alias(\"loan_count\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ca57f5-3138-456e-9a0f-5b7956a71583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_disbursement_efficiency.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"gold_disbursement_efficiency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e947fbd4-6b8a-4051-bc49-55f155f9153a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>branch_id</th><th>branch_name</th><th>avg_disbursement_delay_days</th><th>loan_count</th></tr></thead><tbody><tr><td>BR103</td><td>Indiranagar</td><td>9.0</td><td>5</td></tr><tr><td>BR100</td><td>Andheri</td><td>12.333333333333334</td><td>4</td></tr><tr><td>BR104</td><td>Koramangala</td><td>1.75</td><td>7</td></tr><tr><td>BR102</td><td>Powai</td><td>6.4</td><td>6</td></tr><tr><td>BR106</td><td>Gachibowli</td><td>8.666666666666666</td><td>4</td></tr><tr><td>BR107</td><td>SaltLake</td><td>null</td><td>3</td></tr><tr><td>BR101</td><td>Bandra</td><td>13.0</td><td>2</td></tr><tr><td>BR105</td><td>HitechCity</td><td>5.625</td><td>10</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "BR103",
         "Indiranagar",
         9.0,
         5
        ],
        [
         "BR100",
         "Andheri",
         12.333333333333334,
         4
        ],
        [
         "BR104",
         "Koramangala",
         1.75,
         7
        ],
        [
         "BR102",
         "Powai",
         6.4,
         6
        ],
        [
         "BR106",
         "Gachibowli",
         8.666666666666666,
         4
        ],
        [
         "BR107",
         "SaltLake",
         null,
         3
        ],
        [
         "BR101",
         "Bandra",
         13.0,
         2
        ],
        [
         "BR105",
         "HitechCity",
         5.625,
         10
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "branch_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "branch_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "avg_disbursement_delay_days",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "loan_count",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 61
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "branch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "branch_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_disbursement_delay_days",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "loan_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from gold_disbursement_efficiency;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81614fc6-10fb-4ea5-8d66-e6a42cd8871d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert spark.table(\"gold_customer_360\").count() > 0\n",
    "assert spark.table(\"gold_branch_performance\").count() > 0\n",
    "assert spark.table(\"gold_loan_lifecycle\").count() > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2677e3e4-aef9-486c-925e-32881546ddb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "CreateView"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW vw_customer_360 AS\n",
    "SELECT * FROM gold_customer_360;\n",
    "\n",
    "CREATE OR REPLACE VIEW vw_branch_performance AS\n",
    "SELECT * FROM gold_branch_performance;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f1bafd-bea3-4b0e-9cec-640710f391c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5701564336987453,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver-gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}