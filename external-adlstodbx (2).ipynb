{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0064f659-e105-4e16-aa38-12e7bd40ef78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8018376692618397>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.key.shradls.dfs.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtnrUGc0LKz03/j0gDh329cPtc4sAXqiGB6Bu4b+W/6CBa+X1T+ZHw/qef4k2zZSFrYMDOd/jxQui+AStvWJ4Xw==\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/conf.py:46\u001B[0m, in \u001B[0;36mRuntimeConf.set\u001B[0;34m(self, key, value)\u001B[0m\n",
       "\u001B[1;32m     44\u001B[0m op_set \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mSet(pairs\u001B[38;5;241m=\u001B[39m[proto\u001B[38;5;241m.\u001B[39mKeyValue(key\u001B[38;5;241m=\u001B[39mkey, value\u001B[38;5;241m=\u001B[39mvalue)])\n",
       "\u001B[1;32m     45\u001B[0m operation \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(\u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39mop_set)\n",
       "\u001B[0;32m---> 46\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mconfig(operation)\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m warn \u001B[38;5;129;01min\u001B[39;00m result\u001B[38;5;241m.\u001B[39mwarnings:\n",
       "\u001B[1;32m     48\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(warn)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2166\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   2164\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2166\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [CONFIG_NOT_AVAILABLE] Configuration fs.azure.account.key.shradls.dfs.core.windows.net is not available. SQLSTATE: 42K0I\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:285)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.set(SparkConnectConfigHandler.scala:99)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleSet$1(SparkConnectConfigHandler.scala:267)\n",
       "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
       "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleSet(SparkConnectConfigHandler.scala:264)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:235)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:214)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:214)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:205)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:189)\n",
       "\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:189)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:137)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:874)\n",
       "\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n",
       "\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[CONFIG_NOT_AVAILABLE] Configuration fs.azure.account.key.shradls.dfs.core.windows.net is not available. SQLSTATE: 42K0I\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:285)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.set(SparkConnectConfigHandler.scala:99)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleSet$1(SparkConnectConfigHandler.scala:267)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleSet(SparkConnectConfigHandler.scala:264)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:235)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:214)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:214)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:205)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:189)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:189)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:137)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:874)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[CONFIG_NOT_AVAILABLE] Configuration fs.azure.account.key.shradls.dfs.core.windows.net is not available. SQLSTATE: 42K0I"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "CONFIG_NOT_AVAILABLE",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "42K0I",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:285)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.set(SparkConnectConfigHandler.scala:99)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleSet$1(SparkConnectConfigHandler.scala:267)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleSet(SparkConnectConfigHandler.scala:264)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:235)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:214)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:214)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:205)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:189)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:189)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:137)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:874)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-8018376692618397>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.key.shradls.dfs.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtnrUGc0LKz03/j0gDh329cPtc4sAXqiGB6Bu4b+W/6CBa+X1T+ZHw/qef4k2zZSFrYMDOd/jxQui+AStvWJ4Xw==\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/conf.py:46\u001B[0m, in \u001B[0;36mRuntimeConf.set\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m     44\u001B[0m op_set \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mSet(pairs\u001B[38;5;241m=\u001B[39m[proto\u001B[38;5;241m.\u001B[39mKeyValue(key\u001B[38;5;241m=\u001B[39mkey, value\u001B[38;5;241m=\u001B[39mvalue)])\n\u001B[1;32m     45\u001B[0m operation \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(\u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39mop_set)\n\u001B[0;32m---> 46\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mconfig(operation)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m warn \u001B[38;5;129;01min\u001B[39;00m result\u001B[38;5;241m.\u001B[39mwarnings:\n\u001B[1;32m     48\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(warn)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2166\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   2164\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2166\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [CONFIG_NOT_AVAILABLE] Configuration fs.azure.account.key.shradls.dfs.core.windows.net is not available. SQLSTATE: 42K0I\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:285)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.set(SparkConnectConfigHandler.scala:99)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleSet$1(SparkConnectConfigHandler.scala:267)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleSet(SparkConnectConfigHandler.scala:264)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:235)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:214)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:214)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:205)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:189)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:275)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:189)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:137)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:874)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:389)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:585)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:397)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.shradls.dfs.core.windows.net\", 'key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cde3765-ebeb-4a21-b186-282c34a15a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>branch_id</th><th>customer_id</th><th>interest_rate</th><th>loan_amount</th><th>loan_id</th><th>loan_type</th><th>origination_date</th><th>status</th><th>tenure_months</th><th>update_ts</th></tr></thead><tbody><tr><td>BR105</td><td>C1000</td><td>10.5</td><td>2057234</td><td>L50001</td><td>Personal</td><td>2025-11-26</td><td>APPROVED</td><td>12</td><td>2025-12-01 00:00:00</td></tr><tr><td>BR106</td><td>C1001</td><td>8.5</td><td>1960486</td><td>L50002</td><td>Auto</td><td>2025-11-23</td><td>PENDING</td><td>48</td><td>2025-11-23 00:00:00</td></tr><tr><td>BR106</td><td>C1002</td><td>9.2</td><td>727,030</td><td>L50003</td><td>Auto</td><td>2025-11-18</td><td>PENDING</td><td>48</td><td>2025-11-19 00:00:00</td></tr><tr><td>BR103</td><td>C1003</td><td>8.5</td><td>2421753</td><td>L50004</td><td>Home</td><td>2025-11-21</td><td>PENDING</td><td>60</td><td>2025-11-26 00:00:00</td></tr><tr><td>BR104</td><td>C1004</td><td>10.5</td><td>1,914,865</td><td>L50005</td><td>Auto</td><td>2025-11-13</td><td>PENDING</td><td>12</td><td>2025-11-22 00:00:00</td></tr><tr><td>BR105</td><td>C1005</td><td>11.0</td><td>1,507,901</td><td>L50006</td><td>Personal</td><td>2025-11-17</td><td>PENDING</td><td>24</td><td>2025-11-21 00:00:00</td></tr><tr><td>BR101</td><td>C1006</td><td>12.5</td><td>1435270</td><td>L50007</td><td>Home</td><td>2025-11-18</td><td>APPROVED</td><td>36</td><td>2025-11-22 00:00:00</td></tr><tr><td>BR100</td><td>C1006</td><td>10.5</td><td>947531</td><td>L50008</td><td>Personal</td><td>2025-11-29</td><td>CLOSED</td><td>36</td><td>2025-12-07 00:00:00</td></tr><tr><td>BR102</td><td>C1007</td><td>8.5</td><td>260513</td><td>L50009</td><td>Auto</td><td>2025-11-10</td><td>APPROVED</td><td>60</td><td>2025-11-15 00:00:00</td></tr><tr><td>BR105</td><td>C1008</td><td>11.0</td><td>1,751,301</td><td>L50010</td><td>Education</td><td>2025-11-11</td><td>PENDING</td><td>48</td><td>2025-11-12 00:00:00</td></tr><tr><td>BR104</td><td>C1009</td><td>12.5</td><td>2046383</td><td>L50011</td><td>Auto</td><td>2025-11-09</td><td>PENDING</td><td>48</td><td>2025-11-12 00:00:00</td></tr><tr><td>BR103</td><td>C1010</td><td>9.2</td><td>2,764,833</td><td>L50012</td><td>Education</td><td>2025-11-22</td><td>APPROVED</td><td>12</td><td>2025-11-23 00:00:00</td></tr><tr><td>BR107</td><td>C1011</td><td>10.5</td><td>1238582</td><td>L50013</td><td>Home</td><td>2025-11-20</td><td>APPROVED</td><td>60</td><td>2025-11-29 00:00:00</td></tr><tr><td>BR103</td><td>C1011</td><td>12.5</td><td>767667</td><td>L50014</td><td>Home</td><td>2025-11-12</td><td>PENDING</td><td>48</td><td>2025-11-17 00:00:00</td></tr><tr><td>BR105</td><td>C1012</td><td>12.5</td><td>2,975,871</td><td>L50015</td><td>Personal</td><td>2025-11-09</td><td>DISBURSED</td><td>36</td><td>2025-11-09 00:00:00</td></tr><tr><td>BR105</td><td>C1012</td><td>8.5</td><td>1418271</td><td>L50016</td><td>Education</td><td>2025-11-11</td><td>CLOSED</td><td>60</td><td>2025-11-11 00:00:00</td></tr><tr><td>BR105</td><td>C1013</td><td>8.5</td><td>2694218</td><td>L50017</td><td>Home</td><td>2025-11-30</td><td>DISBURSED</td><td>36</td><td>2025-12-04 00:00:00</td></tr><tr><td>BR103</td><td>C1013</td><td>11.0</td><td>261919</td><td>L50018</td><td>Education</td><td>2025-11-12</td><td>CLOSED</td><td>36</td><td>2025-11-15 00:00:00</td></tr><tr><td>BR104</td><td>C1014</td><td>11.0</td><td>2262528</td><td>L50019</td><td>Auto</td><td>2025-11-04</td><td>PENDING</td><td>12</td><td>2025-11-13 00:00:00</td></tr><tr><td>BR102</td><td>C1014</td><td>8.5</td><td>874,717</td><td>L50020</td><td>Personal</td><td>2025-12-04</td><td>CLOSED</td><td>60</td><td>2025-12-12 00:00:00</td></tr><tr><td>BR100</td><td>C1015</td><td>11.0</td><td>701951</td><td>L50021</td><td>Home</td><td>2025-11-27</td><td>CLOSED</td><td>60</td><td>2025-11-27 00:00:00</td></tr><tr><td>BR104</td><td>C1016</td><td>9.2</td><td>1068906</td><td>L50022</td><td>Personal</td><td>2025-11-10</td><td>APPROVED</td><td>60</td><td>2025-11-14 00:00:00</td></tr><tr><td>BR105</td><td>C1017</td><td>8.5</td><td>602,913</td><td>L50023</td><td>Personal</td><td>2025-11-16</td><td>APPROVED</td><td>24</td><td>2025-11-18 00:00:00</td></tr><tr><td>BR107</td><td>C1017</td><td>11.0</td><td>250,291</td><td>L50024</td><td>Personal</td><td>2025-11-05</td><td>PENDING</td><td>24</td><td>2025-11-10 00:00:00</td></tr><tr><td>BR105</td><td>C1018</td><td>10.5</td><td>2,345,326</td><td>L50025</td><td>Auto</td><td>2025-11-25</td><td>CLOSED</td><td>60</td><td>2025-12-03 00:00:00</td></tr><tr><td>BR102</td><td>C1018</td><td>12.5</td><td>2203139</td><td>L50026</td><td>Personal</td><td>2025-11-01</td><td>DISBURSED</td><td>12</td><td>2025-11-05 00:00:00</td></tr><tr><td>BR104</td><td>C1019</td><td>12.5</td><td>2797175</td><td>L50027</td><td>Auto</td><td>2025-11-20</td><td>PENDING</td><td>24</td><td>2025-11-21 00:00:00</td></tr><tr><td>BR102</td><td>C1020</td><td>10.5</td><td>2056019</td><td>L50028</td><td>Education</td><td>2025-11-18</td><td>APPROVED</td><td>24</td><td>2025-11-27 00:00:00</td></tr><tr><td>BR103</td><td>C1021</td><td>8.5</td><td>269606</td><td>L50029</td><td>Auto</td><td>2025-11-18</td><td>CLOSED</td><td>48</td><td>2025-11-26 00:00:00</td></tr><tr><td>BR102</td><td>C1021</td><td>8.5</td><td>2225303</td><td>L50030</td><td>Education</td><td>2025-11-04</td><td>CLOSED</td><td>12</td><td>2025-11-13 00:00:00</td></tr><tr><td>BR105</td><td>C1022</td><td>12.5</td><td>708111</td><td>L50031</td><td>Home</td><td>2025-11-21</td><td>DISBURSED</td><td>36</td><td>2025-11-25 00:00:00</td></tr><tr><td>BR105</td><td>C1023</td><td>8.5</td><td>2,571,542</td><td>L50032</td><td>Home</td><td>2025-11-28</td><td>CLOSED</td><td>24</td><td>2025-12-06 00:00:00</td></tr><tr><td>BR104</td><td>C1023</td><td>9.2</td><td>2,431,087</td><td>L50033</td><td>Home</td><td>2025-11-09</td><td>CLOSED</td><td>36</td><td>2025-11-16 00:00:00</td></tr><tr><td>BR106</td><td>C1024</td><td>9.2</td><td>2629669</td><td>L50034</td><td>Personal</td><td>2025-12-03</td><td>CLOSED</td><td>36</td><td>2025-12-04 00:00:00</td></tr><tr><td>BR100</td><td>C1024</td><td>10.5</td><td>2794122</td><td>L50035</td><td>Personal</td><td>2025-11-14</td><td>PENDING</td><td>60</td><td>2025-11-22 00:00:00</td></tr><tr><td>BR104</td><td>C1025</td><td>9.2</td><td>1,686,846</td><td>L50036</td><td>Education</td><td>2025-11-16</td><td>DISBURSED</td><td>60</td><td>2025-11-23 00:00:00</td></tr><tr><td>BR106</td><td>C1026</td><td>8.5</td><td>2,885,601</td><td>L50037</td><td>Home</td><td>2025-11-29</td><td>APPROVED</td><td>36</td><td>2025-12-04 00:00:00</td></tr><tr><td>BR107</td><td>C1026</td><td>9.2</td><td>609285</td><td>L50038</td><td>Home</td><td>2025-11-13</td><td>APPROVED</td><td>24</td><td>2025-11-22 00:00:00</td></tr><tr><td>BR102</td><td>C1027</td><td>8.5</td><td>2,778,717</td><td>L50039</td><td>Education</td><td>2025-12-03</td><td>PENDING</td><td>24</td><td>2025-12-08 00:00:00</td></tr><tr><td>BR101</td><td>C1028</td><td>11.0</td><td>523,372</td><td>L50040</td><td>Personal</td><td>2025-11-16</td><td>APPROVED</td><td>12</td><td>2025-11-17 00:00:00</td></tr><tr><td>BR100</td><td>C1029</td><td>10.5</td><td>349469</td><td>L50041</td><td>Education</td><td>2025-11-30</td><td>DISBURSED</td><td>48</td><td>2025-12-08 00:00:00</td></tr><tr><td>BR103</td><td>null</td><td>11.0</td><td>500,000</td><td>L50042</td><td>Personal</td><td>2025-11-04</td><td>APPROVED</td><td>24</td><td>2025-11-05 00:00:00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "BR105",
         "C1000",
         10.5,
         "2057234",
         "L50001",
         "Personal",
         "2025-11-26",
         "APPROVED",
         12,
         "2025-12-01 00:00:00"
        ],
        [
         "BR106",
         "C1001",
         8.5,
         "1960486",
         "L50002",
         "Auto",
         "2025-11-23",
         "PENDING",
         48,
         "2025-11-23 00:00:00"
        ],
        [
         "BR106",
         "C1002",
         9.2,
         "727,030",
         "L50003",
         "Auto",
         "2025-11-18",
         "PENDING",
         48,
         "2025-11-19 00:00:00"
        ],
        [
         "BR103",
         "C1003",
         8.5,
         "2421753",
         "L50004",
         "Home",
         "2025-11-21",
         "PENDING",
         60,
         "2025-11-26 00:00:00"
        ],
        [
         "BR104",
         "C1004",
         10.5,
         "1,914,865",
         "L50005",
         "Auto",
         "2025-11-13",
         "PENDING",
         12,
         "2025-11-22 00:00:00"
        ],
        [
         "BR105",
         "C1005",
         11.0,
         "1,507,901",
         "L50006",
         "Personal",
         "2025-11-17",
         "PENDING",
         24,
         "2025-11-21 00:00:00"
        ],
        [
         "BR101",
         "C1006",
         12.5,
         "1435270",
         "L50007",
         "Home",
         "2025-11-18",
         "APPROVED",
         36,
         "2025-11-22 00:00:00"
        ],
        [
         "BR100",
         "C1006",
         10.5,
         "947531",
         "L50008",
         "Personal",
         "2025-11-29",
         "CLOSED",
         36,
         "2025-12-07 00:00:00"
        ],
        [
         "BR102",
         "C1007",
         8.5,
         "260513",
         "L50009",
         "Auto",
         "2025-11-10",
         "APPROVED",
         60,
         "2025-11-15 00:00:00"
        ],
        [
         "BR105",
         "C1008",
         11.0,
         "1,751,301",
         "L50010",
         "Education",
         "2025-11-11",
         "PENDING",
         48,
         "2025-11-12 00:00:00"
        ],
        [
         "BR104",
         "C1009",
         12.5,
         "2046383",
         "L50011",
         "Auto",
         "2025-11-09",
         "PENDING",
         48,
         "2025-11-12 00:00:00"
        ],
        [
         "BR103",
         "C1010",
         9.2,
         "2,764,833",
         "L50012",
         "Education",
         "2025-11-22",
         "APPROVED",
         12,
         "2025-11-23 00:00:00"
        ],
        [
         "BR107",
         "C1011",
         10.5,
         "1238582",
         "L50013",
         "Home",
         "2025-11-20",
         "APPROVED",
         60,
         "2025-11-29 00:00:00"
        ],
        [
         "BR103",
         "C1011",
         12.5,
         "767667",
         "L50014",
         "Home",
         "2025-11-12",
         "PENDING",
         48,
         "2025-11-17 00:00:00"
        ],
        [
         "BR105",
         "C1012",
         12.5,
         "2,975,871",
         "L50015",
         "Personal",
         "2025-11-09",
         "DISBURSED",
         36,
         "2025-11-09 00:00:00"
        ],
        [
         "BR105",
         "C1012",
         8.5,
         "1418271",
         "L50016",
         "Education",
         "2025-11-11",
         "CLOSED",
         60,
         "2025-11-11 00:00:00"
        ],
        [
         "BR105",
         "C1013",
         8.5,
         "2694218",
         "L50017",
         "Home",
         "2025-11-30",
         "DISBURSED",
         36,
         "2025-12-04 00:00:00"
        ],
        [
         "BR103",
         "C1013",
         11.0,
         "261919",
         "L50018",
         "Education",
         "2025-11-12",
         "CLOSED",
         36,
         "2025-11-15 00:00:00"
        ],
        [
         "BR104",
         "C1014",
         11.0,
         "2262528",
         "L50019",
         "Auto",
         "2025-11-04",
         "PENDING",
         12,
         "2025-11-13 00:00:00"
        ],
        [
         "BR102",
         "C1014",
         8.5,
         "874,717",
         "L50020",
         "Personal",
         "2025-12-04",
         "CLOSED",
         60,
         "2025-12-12 00:00:00"
        ],
        [
         "BR100",
         "C1015",
         11.0,
         "701951",
         "L50021",
         "Home",
         "2025-11-27",
         "CLOSED",
         60,
         "2025-11-27 00:00:00"
        ],
        [
         "BR104",
         "C1016",
         9.2,
         "1068906",
         "L50022",
         "Personal",
         "2025-11-10",
         "APPROVED",
         60,
         "2025-11-14 00:00:00"
        ],
        [
         "BR105",
         "C1017",
         8.5,
         "602,913",
         "L50023",
         "Personal",
         "2025-11-16",
         "APPROVED",
         24,
         "2025-11-18 00:00:00"
        ],
        [
         "BR107",
         "C1017",
         11.0,
         "250,291",
         "L50024",
         "Personal",
         "2025-11-05",
         "PENDING",
         24,
         "2025-11-10 00:00:00"
        ],
        [
         "BR105",
         "C1018",
         10.5,
         "2,345,326",
         "L50025",
         "Auto",
         "2025-11-25",
         "CLOSED",
         60,
         "2025-12-03 00:00:00"
        ],
        [
         "BR102",
         "C1018",
         12.5,
         "2203139",
         "L50026",
         "Personal",
         "2025-11-01",
         "DISBURSED",
         12,
         "2025-11-05 00:00:00"
        ],
        [
         "BR104",
         "C1019",
         12.5,
         "2797175",
         "L50027",
         "Auto",
         "2025-11-20",
         "PENDING",
         24,
         "2025-11-21 00:00:00"
        ],
        [
         "BR102",
         "C1020",
         10.5,
         "2056019",
         "L50028",
         "Education",
         "2025-11-18",
         "APPROVED",
         24,
         "2025-11-27 00:00:00"
        ],
        [
         "BR103",
         "C1021",
         8.5,
         "269606",
         "L50029",
         "Auto",
         "2025-11-18",
         "CLOSED",
         48,
         "2025-11-26 00:00:00"
        ],
        [
         "BR102",
         "C1021",
         8.5,
         "2225303",
         "L50030",
         "Education",
         "2025-11-04",
         "CLOSED",
         12,
         "2025-11-13 00:00:00"
        ],
        [
         "BR105",
         "C1022",
         12.5,
         "708111",
         "L50031",
         "Home",
         "2025-11-21",
         "DISBURSED",
         36,
         "2025-11-25 00:00:00"
        ],
        [
         "BR105",
         "C1023",
         8.5,
         "2,571,542",
         "L50032",
         "Home",
         "2025-11-28",
         "CLOSED",
         24,
         "2025-12-06 00:00:00"
        ],
        [
         "BR104",
         "C1023",
         9.2,
         "2,431,087",
         "L50033",
         "Home",
         "2025-11-09",
         "CLOSED",
         36,
         "2025-11-16 00:00:00"
        ],
        [
         "BR106",
         "C1024",
         9.2,
         "2629669",
         "L50034",
         "Personal",
         "2025-12-03",
         "CLOSED",
         36,
         "2025-12-04 00:00:00"
        ],
        [
         "BR100",
         "C1024",
         10.5,
         "2794122",
         "L50035",
         "Personal",
         "2025-11-14",
         "PENDING",
         60,
         "2025-11-22 00:00:00"
        ],
        [
         "BR104",
         "C1025",
         9.2,
         "1,686,846",
         "L50036",
         "Education",
         "2025-11-16",
         "DISBURSED",
         60,
         "2025-11-23 00:00:00"
        ],
        [
         "BR106",
         "C1026",
         8.5,
         "2,885,601",
         "L50037",
         "Home",
         "2025-11-29",
         "APPROVED",
         36,
         "2025-12-04 00:00:00"
        ],
        [
         "BR107",
         "C1026",
         9.2,
         "609285",
         "L50038",
         "Home",
         "2025-11-13",
         "APPROVED",
         24,
         "2025-11-22 00:00:00"
        ],
        [
         "BR102",
         "C1027",
         8.5,
         "2,778,717",
         "L50039",
         "Education",
         "2025-12-03",
         "PENDING",
         24,
         "2025-12-08 00:00:00"
        ],
        [
         "BR101",
         "C1028",
         11.0,
         "523,372",
         "L50040",
         "Personal",
         "2025-11-16",
         "APPROVED",
         12,
         "2025-11-17 00:00:00"
        ],
        [
         "BR100",
         "C1029",
         10.5,
         "349469",
         "L50041",
         "Education",
         "2025-11-30",
         "DISBURSED",
         48,
         "2025-12-08 00:00:00"
        ],
        [
         "BR103",
         null,
         11.0,
         "500,000",
         "L50042",
         "Personal",
         "2025-11-04",
         "APPROVED",
         24,
         "2025-11-05 00:00:00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "branch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "interest_rate",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "loan_amount",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loan_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loan_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "origination_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tenure_months",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "update_ts",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"abfss://landing@shradls.dfs.core.windows.net/loans.json\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "external-adlstodbx",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}